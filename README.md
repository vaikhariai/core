<div align="center">

## CORE: *Comprehensive Ontological Relation Evaluation*

</div>

**CORE** is the **first benchmark of its kind** focused explicitly on **ontological and semantic reasoning**, covering a **broad majority of known ontological and semantic relations** used in human reasoning. Unlike surface-level QA or task-oriented benchmarks, CORE is designed to evaluate whether models truly understand **how concepts relate to one another**, rather than merely recalling facts or patterns.

CORE evaluates reasoning across relations such as (non-exhaustive):

* cause–effect
* part–whole
* function–object
* spatial, temporal, and logical relations
* related vs unrelated concept distinctions

This makes CORE particularly suitable for:

* evaluating reasoning robustness,
* identifying overconfidence and calibration failures,
* comparing human vs model consensus behavior,
* stress-testing models on *meaning*, not memorization.